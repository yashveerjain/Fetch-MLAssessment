{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\yashv/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "dataset = TaskDataset(data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Sentence Embedding Model, architecture:\n",
    " - Embedding Layer \n",
    " - Positional Encoding\n",
    " - Transformer Encoder Layer\n",
    " - Mean Pooling\n",
    " - Linear Layer (generates embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models\n",
    "n_layers = 1 # The number of transformer encoder layers\n",
    "d_model = 512 # The input embedding dimension\n",
    "embed_size = 300 # The output embedding size\n",
    "nhead = 8 # The number of attention heads in encoder layer\n",
    "task_type = \"se\" # The type of task (sentence embedding or sentiment classification)\n",
    "vocab_size = dataset.vocab_size # The size of the vocabulary (using hugging face bert tokenizer)\n",
    "base_model, se_task_model = init_model(vocab_size, n_layers, d_model, nhead, embed_size, task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output: torch.Size([2, 100, 512])\n",
      "Task model output: torch.Size([2, 300])\n",
      "Base model output 1: torch.Size([1, 7, 512])\n",
      "Base model output 2: torch.Size([1, 5, 512])\n",
      "Task model output 1: torch.Size([1, 300])\n",
      "Task model output 2: torch.Size([1, 300])\n",
      "Task model output 2: tensor([[ 2.9115e-02,  3.7576e-03,  1.6875e-02, -4.3329e-02, -2.3840e-02,\n",
      "         -3.0817e-02,  4.4260e-02,  4.8819e-02, -5.7285e-02, -5.8882e-02,\n",
      "         -4.9020e-02, -1.0446e-01,  1.0870e-03, -1.1216e-02, -3.4832e-02,\n",
      "         -4.2444e-02, -1.3106e-02, -6.3622e-03, -6.6139e-02,  2.4911e-02,\n",
      "         -2.6597e-02,  8.5434e-02,  8.7531e-02,  6.6500e-02,  6.1909e-02,\n",
      "         -9.4715e-02,  6.7242e-02, -5.1707e-02, -5.5728e-02, -5.6037e-03,\n",
      "         -3.4221e-02,  8.2079e-02, -3.9183e-03, -5.8341e-02, -2.9514e-02,\n",
      "          2.6526e-02,  8.2539e-02,  9.0273e-02,  6.7931e-03,  2.2955e-02,\n",
      "          3.4709e-02,  7.3479e-02, -7.0017e-02,  4.3275e-02,  9.1688e-02,\n",
      "         -6.9128e-02,  1.9761e-02, -7.2405e-02,  8.2110e-03, -5.8195e-03,\n",
      "         -9.6660e-02,  3.0819e-03,  2.4287e-02, -5.4164e-02,  4.1349e-02,\n",
      "          3.9862e-02,  3.9504e-02,  6.5227e-03, -4.4158e-02, -1.8180e-01,\n",
      "          4.3292e-02, -1.2049e-01,  1.4679e-02, -1.1008e-01, -6.5993e-02,\n",
      "          4.0613e-02,  7.7858e-02,  1.9354e-02, -2.8743e-02,  4.3380e-02,\n",
      "         -1.8538e-03,  1.4816e-03, -3.6932e-02, -1.2550e-01, -8.8876e-02,\n",
      "         -4.8385e-02,  2.7787e-02,  1.1739e-01, -2.5186e-02, -2.0161e-02,\n",
      "         -9.3415e-02,  4.3349e-02, -2.8985e-03,  6.8162e-03,  1.3548e-01,\n",
      "          7.9303e-02, -3.5446e-02,  4.0610e-02,  4.5554e-02,  8.0662e-02,\n",
      "          2.7490e-02, -3.4149e-02, -3.4577e-02,  3.0255e-02, -3.7439e-03,\n",
      "          3.4665e-02,  8.2560e-03, -1.9208e-03, -7.7647e-02,  3.9142e-02,\n",
      "         -1.6922e-02,  1.9528e-02,  3.1733e-02,  2.9357e-02, -5.5626e-03,\n",
      "          5.0997e-02, -1.5846e-02,  5.9768e-02, -4.2186e-02,  3.5631e-02,\n",
      "         -1.2157e-01,  5.0689e-02, -5.2243e-02,  1.3141e-02, -7.3061e-02,\n",
      "         -3.8997e-02, -2.9718e-03, -1.1490e-01, -1.8233e-02,  4.6223e-02,\n",
      "          3.7053e-03,  6.5021e-02, -4.5240e-02, -6.7610e-02, -4.0795e-02,\n",
      "          7.3259e-04, -1.4180e-02,  9.3844e-02,  8.1072e-03, -4.7948e-03,\n",
      "          9.2346e-02, -6.6551e-02,  3.9900e-02,  1.0937e-01, -1.4121e-01,\n",
      "          3.2588e-02, -4.3656e-02, -2.8658e-02, -1.5561e-02,  6.3222e-02,\n",
      "         -9.6672e-02,  6.7739e-02, -8.1196e-03, -5.3829e-02, -3.3262e-02,\n",
      "          7.1026e-02, -4.5212e-03,  1.2839e-02,  1.2046e-02,  1.6367e-02,\n",
      "          6.0768e-03, -3.3072e-02,  3.7178e-02, -5.5277e-02,  2.0965e-02,\n",
      "          9.3141e-02, -2.9385e-02, -9.3786e-02,  1.3341e-02,  8.3808e-02,\n",
      "         -5.8361e-02,  4.3396e-02,  2.0889e-02,  5.2650e-03,  6.4641e-02,\n",
      "         -3.6066e-02, -2.6902e-02,  4.2333e-02,  7.9581e-02, -3.4541e-02,\n",
      "         -1.9160e-02,  2.7017e-02,  1.6612e-02,  3.2879e-02,  3.1592e-02,\n",
      "          3.2100e-02, -2.3239e-02, -7.6529e-02, -2.9884e-02,  5.1286e-02,\n",
      "          1.8151e-01,  3.8750e-02,  2.3006e-02, -7.6440e-02,  1.1994e-02,\n",
      "          6.9047e-02,  2.5742e-02, -4.6813e-02, -6.4697e-02, -4.0547e-02,\n",
      "          2.7066e-02,  3.4921e-02,  1.2212e-01,  4.1300e-02, -1.9900e-02,\n",
      "          2.3842e-04, -3.0694e-02, -1.5188e-02, -5.7887e-02, -2.7648e-02,\n",
      "         -6.0115e-03, -1.5276e-02,  3.2848e-02,  6.1509e-03, -1.0304e-02,\n",
      "         -4.2005e-02,  5.4734e-02, -6.4646e-02, -5.2495e-02, -3.7805e-02,\n",
      "          3.6722e-02,  6.6713e-02,  6.1006e-02,  8.8674e-02, -5.4817e-03,\n",
      "          6.4305e-03, -1.0816e-01, -2.5301e-02, -1.0598e-02, -3.1205e-02,\n",
      "          2.9708e-02, -5.2632e-02,  7.2824e-02, -3.5828e-02,  4.9487e-02,\n",
      "         -4.1309e-02,  6.8895e-02, -4.8755e-02, -3.1727e-02,  7.9452e-02,\n",
      "         -6.3537e-02, -1.7193e-02, -5.5488e-02,  6.4476e-02,  6.2916e-02,\n",
      "          7.1388e-02, -4.3662e-02, -2.0674e-02, -3.2392e-03,  3.9058e-02,\n",
      "         -7.5086e-02, -6.8118e-02,  7.3409e-02,  6.2351e-03,  1.0734e-01,\n",
      "          3.7092e-02, -8.3218e-02, -1.4215e-02, -1.9695e-02, -6.5437e-02,\n",
      "         -4.2576e-02,  4.3057e-02,  9.8378e-03, -8.6356e-03,  6.5708e-02,\n",
      "          9.2038e-03,  8.3096e-03, -1.8450e-02, -5.5100e-02, -4.0544e-02,\n",
      "          6.6974e-02, -3.7801e-02, -3.5962e-02,  1.4054e-02, -2.4725e-01,\n",
      "         -3.4661e-02, -7.6324e-02, -1.2277e-01,  3.4132e-04, -1.0609e-01,\n",
      "         -8.8488e-02,  5.2548e-02,  1.6225e-02,  1.1577e-02, -8.1814e-03,\n",
      "         -1.0148e-01, -1.3200e-02,  7.3093e-02,  8.7704e-03,  1.1977e-02,\n",
      "          3.4109e-02, -2.4212e-02, -9.8625e-03, -6.3127e-02,  4.4867e-02,\n",
      "         -1.7629e-03,  3.8443e-02, -4.1843e-02, -5.9567e-02, -9.7701e-03,\n",
      "         -1.1268e-01, -9.7206e-02, -4.1389e-02, -9.0785e-02, -2.0727e-02,\n",
      "         -2.9252e-02,  6.1840e-02,  1.5232e-01,  2.8547e-02, -3.8812e-02]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample input\n",
    "input_data = torch.randint(0, dataset.vocab_size, (2, 100))\n",
    "\n",
    "# Forward pass\n",
    "base_output = base_model(input_data)\n",
    "task_output = se_task_model(base_output)\n",
    "\n",
    "print(\"Base model output:\", base_output.shape)\n",
    "print(\"Task model output:\", task_output.shape)\n",
    "\n",
    "# Generate input based on text\n",
    "text1 = \"How, are you?\"\n",
    "text2 = \"I am good\"\n",
    "\n",
    "input_data1 = dataset.encode(text1)\n",
    "input_data2 = dataset.encode(text2)\n",
    "\n",
    "# Forward pass\n",
    "base_output1 = base_model(input_data1)\n",
    "base_output2 = base_model(input_data2)\n",
    "task_output1 = se_task_model(base_output1)\n",
    "task_output2 = se_task_model(base_output2)\n",
    "\n",
    "print(\"Base model output 1:\", base_output1.shape)\n",
    "print(\"Base model output 2:\", base_output2.shape)\n",
    "print(\"Task model output 1:\", task_output1.shape)   \n",
    "print(\"Task model output 2:\", task_output2.shape)\n",
    "print(\"Task model output 2:\", task_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Sentiment Classification Model, architecture:\n",
    " - Embedding Layer\n",
    " - Positional Encoding\n",
    " - Transformer Encoder Layer\n",
    " - Mean Pooling\n",
    " - Linear Layer (3 classes, Positive, Negative, Neutral) ( can be trained using cross entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = \"sc\"\n",
    "base_model, sc_task_model = init_model(vocab_size, n_layers, d_model, nhead, embed_size, task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output: torch.Size([2, 100, 512])\n",
      "Task model output: (2, 3)\n",
      "Task model output: [[0.32383433 0.38363796 0.2925277 ]\n",
      " [0.32741138 0.39954418 0.27304447]]\n",
      "Base model output 1: torch.Size([1, 7, 512])\n",
      "Base model output 2: torch.Size([1, 5, 512])\n",
      "Task model output 1: (1, 3)\n",
      "Task model output 2: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample input\n",
    "input_data = torch.randint(0, dataset.vocab_size, (2, 100))\n",
    "\n",
    "# Forward pass\n",
    "base_output = base_model(input_data)\n",
    "task_output = sc_task_model(base_output)\n",
    "task_output = nn.Softmax(dim=1)(task_output).detach().numpy()\n",
    "\n",
    "print(\"Base model output:\", base_output.shape)\n",
    "print(\"Task model output:\", task_output.shape)\n",
    "print(\"Task model output:\", task_output)\n",
    "\n",
    "# Generate input based on text\n",
    "text1 = \"How, are you?\"\n",
    "text2 = \"I am good\"\n",
    "\n",
    "input_data1 = dataset.encode(text1)\n",
    "input_data2 = dataset.encode(text2)\n",
    "\n",
    "# Forward pass\n",
    "base_output1 = base_model(input_data1)\n",
    "base_output2 = base_model(input_data2)\n",
    "task_output1 = sc_task_model(base_output1)\n",
    "task_output2 = sc_task_model(base_output2)\n",
    "\n",
    "task_output1 = nn.Softmax(dim=1)(task_output1).detach().numpy()\n",
    "task_output2 = nn.Softmax(dim=1)(task_output2).detach().numpy()\n",
    "\n",
    "print(\"Base model output 1:\", base_output1.shape)\n",
    "print(\"Base model output 2:\", base_output2.shape)\n",
    "print(\"Task model output 1:\", task_output1.shape)   \n",
    "print(\"Task model output 2:\", task_output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Assuming we are training the sentence embedding model, and the sentiment classification model, simulatenously. Then each model can have different learning rates, as well for embedding in transformer will add different learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding Layer: Set a learning rate of 1e-3.\n",
    "  \n",
    "  -  This is a low-level layer responsible for capturing foundational features, so a finer learning rate helps prevent drastic changes that could drift away from good initial representations.\n",
    "* Transformer Layer: Use a learning rate of 5e-3.\n",
    "   \n",
    "   - As a middle layer, this part of the model focuses on generalizing patterns from the dataset. A slightly higher learning rate allows it to adapt faster, helping it develop better generalizations over time.\n",
    "* Sentence Embedding and Sentiment Classification Layers: Set a learning rate of 1e-2.\n",
    "  - These layers capture task-specific, high-level features crucial for task understanding. A higher learning rate here allows faster learning of these details, which is important to avoid long convergence times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of BaseModel(\n",
      "  (embed): Embedding(30522, 512)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pos_embed): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(base_model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "base_model_transformer_lr = 5e-3\n",
    "base_model_embedding_lr = 1e-3\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    if \"embed\" in name:\n",
    "        parameters.append({\"params\": param, \"lr\": base_model_embedding_lr})\n",
    "    else:\n",
    "        parameters.append({\"params\": param, \"lr\": base_model_transformer_lr})\n",
    "\n",
    "se_lr = 1e-2\n",
    "sc_lr = 1e-2\n",
    "\n",
    "parameters.append(\n",
    "    {\"params\" : se_task_model.parameters(), \"lr\" : se_lr}\n",
    ")\n",
    "\n",
    "parameters.append(\n",
    "    {\"params\" : sc_task_model.parameters(), \"lr\" : sc_lr}\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
